{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T19:11:09.411942Z",
     "start_time": "2025-03-30T19:11:08.928221Z"
    }
   },
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T19:23:30.155753Z",
     "start_time": "2025-03-30T19:23:28.342003Z"
    }
   },
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T19:23:52.338511Z",
     "start_time": "2025-03-30T19:23:52.322927Z"
    }
   },
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape, # train_df.shape,"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Setup LLM Pipeline"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T19:46:28.904614Z",
     "start_time": "2025-03-30T19:46:27.024365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from decouple import config\n",
    "from dotenv import load_dotenv\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "WX_API_KEY = config('WX_API_KEY')\n",
    "\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\", # using Dallas region as the doc specified to do so\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials,\n",
    "    project_id=\"933e6007-4781-432a-9591-21b932da4bcb\"\n",
    ")\n",
    "\n",
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "def get_model(model_id: str):\n",
    "    \"\"\" Get the model from the API \"\"\"\n",
    "    return ModelInference(\n",
    "        api_client=client,\n",
    "        model_id=model_id,\n",
    "        params=PARAMS\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Create a system prompt"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T20:15:15.411334Z",
     "start_time": "2025-03-30T20:15:15.407641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SYSTEM_PROMPT = \"\"\"Your task is to classify news stories into one of the four following categories.\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Example 1:\n",
    "TEXT: LEGO Group reported record-breaking revenue for Q4, driven by strong holiday sales and demand for licensed sets.\n",
    "Category: Business\n",
    "\n",
    "Example 2:\n",
    "TEXT: Apple unveiled its latest line of MacBook Pros featuring the new M3 chip, promising faster performance and improved battery life.\n",
    "Category: Sci/Tech\n",
    "\n",
    "Example 3:\n",
    "TEXT: Manchester United narrowly defeated Liverpool in a thrilling 3-2 match at Old Trafford.\n",
    "Category: Sports\n",
    "\n",
    "Example 4:\n",
    "TEXT: The European Union has proposed new regulations to combat climate change, aiming for a 55% reduction in emissions by 2030.\n",
    "Category: World\n",
    "\n",
    "Now classify the following news story:\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else. So only the category specified in the examples above, where it is most fitting.\n",
    "\n",
    "Category:\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Generate predictions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T20:24:12.087508Z",
     "start_time": "2025-03-30T20:15:21.032446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "# ibm/granite model\n",
    "ibm_model = get_model(\"ibm/granite-13b-instruct-v2\")\n",
    "predictions_ibm_granite = []\n",
    "\n",
    "# meta/llama model\n",
    "meta_model = get_model(\"meta-llama/llama-3-405b-instruct\")\n",
    "predictions_meta_llama = []\n",
    "\n",
    "models = [ibm_model, meta_model]\n",
    "\n",
    "for model in models:\n",
    "    # Array to store predictions for each model\n",
    "    predictions = []\n",
    "\n",
    "    # Train on all models in model ids\n",
    "    for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "        # format the prompt with the categories and the text\n",
    "        prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "\n",
    "        # generate the response from the model\n",
    "        response = model.generate(prompt)\n",
    "\n",
    "        # extract the generated text from the response\n",
    "        prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "        # append the prediction to the list of predictions\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Store the predictions in the correct variable\n",
    "    if model.model_id == \"ibm/granite-13b-instruct-v2\":\n",
    "        predictions_ibm_granite = predictions\n",
    "    elif model.model_id == \"meta-llama/llama-3-405b-instruct\":\n",
    "        predictions_meta_llama = predictions"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [04:29<00:00,  2.82it/s]\n",
      "100%|██████████| 760/760 [04:19<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Evaluate performance"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T20:26:25.356684Z",
     "start_time": "2025-03-30T20:26:25.335941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"IBM Granite\")\n",
    "print(classification_report(test_df.label, predictions_ibm_granite))\n",
    "print(\"Meta Llama\")\n",
    "print(classification_report(test_df.label, predictions_meta_llama))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM Granite\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Business       0.55      0.93      0.69       190\n",
      "    Interview       0.00      0.00      0.00         0\n",
      "          Law       0.00      0.00      0.00         0\n",
      "Miscellaneous       0.00      0.00      0.00         0\n",
      "     Sci/Tech       0.87      0.44      0.58       190\n",
      "        Space       0.00      0.00      0.00         0\n",
      "       Sports       0.93      0.92      0.92       190\n",
      "          War       0.00      0.00      0.00         0\n",
      "        World       0.86      0.67      0.75       190\n",
      "\n",
      "     accuracy                           0.74       760\n",
      "    macro avg       0.36      0.33      0.33       760\n",
      " weighted avg       0.80      0.74      0.74       760\n",
      "\n",
      "Meta Llama\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "(Your answer here)       0.00      0.00      0.00         0\n",
      "                 ?       0.00      0.00      0.00         0\n",
      "          Business       0.78      0.92      0.84       190\n",
      "          Sci/Tech       0.92      0.78      0.84       190\n",
      "            Sports       0.97      0.97      0.97       190\n",
      "             World       0.93      0.88      0.90       190\n",
      "               ```       0.00      0.00      0.00         0\n",
      "\n",
      "          accuracy                           0.89       760\n",
      "         macro avg       0.51      0.51      0.51       760\n",
      "      weighted avg       0.90      0.89      0.89       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christian/miniconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christian/miniconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christian/miniconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christian/miniconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christian/miniconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/christian/miniconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Reflections\n",
    "\n",
    "As a result, we got an accuracy of 0.89 for the Meta Llama model, and 0.74 for the IBM Granite model. To compare the models from Part I and II, considering speed performance, the Meta Llama model is faster and better than the BERT model. The BoW model still has the best results (though it is also trained on more data) but is a few minutes slower than the LLM Llama model.\n",
    "\n",
    "In our first iteration of our ´SYSTEM_PROMPT´, the IBM model return a lot of other categories we haven't defined, even though we made use of few shot prompting. After trying to specify that it should only return the categories we defined, it started to work a little bit better but still added other categories.\n",
    "\n",
    "The Llama model was better at sticking to categories. So for the task of categorising news articles, the Llama model could be a quicker solution with a few percent differences to the BoW, but if accuracy is the most important, the BoW model is assumably the better option based on our assignments, though if time allowed we would be more sure by testing the Llama model with more data, as the BoW model was trained on all of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
